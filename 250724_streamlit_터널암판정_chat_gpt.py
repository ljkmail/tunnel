import streamlit as st
import os
import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
from PIL import Image
from torchvision import transforms
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image
from dotenv import load_dotenv
import base64
import httpx
from openai import OpenAI
import urllib3
import tempfile
import gc

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
load_dotenv()
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    http_client=httpx.Client(verify=False)
)

# ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_model():
    model = torch.load("./model_convnext_tiny2.pth", weights_only=False, map_location=torch.device('cpu'))
    model.eval()
    return model

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
def preprocess(image: Image.Image):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])
    return transform(image).unsqueeze(0)

# PIL â†’ base64
def pil_to_base64(image: Image.Image) -> str:
    from io import BytesIO
    buffer = BytesIO()
    image.save(buffer, format="PNG")
    return base64.b64encode(buffer.getvalue()).decode("utf-8")

# Grad-CAM ê³„ì‚°
def generate_gradcam(model, input_tensor, original_np):
    target_layer = model.features[-1][-1]
    cam = GradCAM(model=model, target_layers=[target_layer])
    grayscale_cam = cam(input_tensor=input_tensor)[0]
    cam_image = show_cam_on_image(original_np, grayscale_cam, use_rgb=True)
    return cam_image

# GPT5-mini ë¶„ì„
def analyze_with_gpt(original_img: Image.Image, cam_img: Image.Image, label_idx_: int, class_name: str) -> str:
    original_base64 = pil_to_base64(original_img)
    cam_base64 = pil_to_base64(cam_img)

    prompt = (
        f"ì´ ì´ë¯¸ì§€ëŠ” RMR ì•”ë°˜ ë¶„ë¥˜ ì¤‘ Class {label_idx_} ({class_name})ë¡œ ì˜ˆì¸¡ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        "ë‘ ì´ë¯¸ì§€ë¥¼ ì°¸ê³ í•˜ì—¬ Grad-CAM ê²°ê³¼ë¥¼ ë°˜ì˜í•œ ê¸°ìˆ ì  ë¶„ì„ì„ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤.\n\n"
        "ë‹¤ìŒ í•­ëª©ë³„ë¡œ í‘œ í˜•íƒœ(Markdown Table)ë¡œ ì •ë¦¬í•´ì„œ ë‹µë³€í•˜ì„¸ìš”:\n"
        "1. ì•”ë°˜ í‘œë©´ í’í™” ìƒíƒœ\n"
        "2. ì ˆë¦¬ ê°„ê²© ë° ë°©í–¥ì„±\n"
        "3. ì•”ì„ ê°•ë„\n"
        "4. êµ¬ì¡°ì  ì•ˆì •ì„±\n"
        "5. Grad-CAM ê°•ì¡° ì˜ì—­ì˜ ì§€ì§ˆí•™ì  ì˜ë¯¸\n\n"
        "ì¶œë ¥ì€ ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì€ í‘œ í˜•ì‹ìœ¼ë¡œ í•´ì£¼ì„¸ìš”:\n"
        "| í•­ëª© | ë¶„ì„ ê²°ê³¼ |\n"
        "|------|-----------|\n"
        "ë§¨ì•ì—ëŠ” ìœ„ì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì„œ ì¶œë ¥í•´ ì£¼ì„¸ìš”\n"
        "í•„ìš”í•˜ì‹œë©´ ë˜ëŠ” ì›í•˜ì‹ ë‹¤ë©´ ì–´ë–¤ê²ƒì„ ì‘ì„±í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤ëŠ” ì œì™¸í•˜ê³  ë‹µë³€"
    )

    response = client.chat.completions.create(
        model="gpt-5-mini",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì§€ì§ˆê³µí•™ ë° ì•”ë°˜ê³µí•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{original_base64}"}},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{cam_base64}"}}
                ]
            }
        ],
        temperature=1
    )
    return response.choices[0].message.content

# Streamlit UI
st.title("í„°ë„ ì•ˆì „ì„± AIğŸ¤– ë¶„ë¥˜ ì„œë¹„ìŠ¤")

uploaded_file = st.file_uploader("í„°ë„ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "png", "jpeg"])

if uploaded_file:
    # ì„ì‹œ íŒŒì¼ ì €ì¥
    temp_file = tempfile.NamedTemporaryFile(delete=False)
    temp_file.write(uploaded_file.getvalue())
    image_path = temp_file.name

    # ì´ë¯¸ì§€ ë¡œë”©
    image = Image.open(image_path).convert("RGB")
    st.image(image, caption="ì—…ë¡œë“œëœ ì›ë³¸ ì´ë¯¸ì§€", use_container_width=True)

    # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    with st.spinner("ëª¨ë¸ ë¡œë”© ë° ì˜ˆì¸¡ ì¤‘..."):
        model = load_model()
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        input_tensor = preprocess(image).to(device)
        original_np = np.array(image.resize((224, 224))).astype(np.float32) / 255.0

        # ===== ì˜ˆì¸¡ (í´ë˜ìŠ¤ë³„ í™•ë¥  í¬í•¨) =====
        with torch.no_grad():
            outputs = model(input_tensor)
            probs = F.softmax(outputs, dim=1).cpu().numpy()[0]  # í´ë˜ìŠ¤ë³„ í™•ë¥ 

        pred = np.argmax(probs) + 1  # ê°€ì¥ ë†’ì€ í™•ë¥  í´ë˜ìŠ¤ (ì¸ë±ìŠ¤ + 1)

        rmr_classes = {
            1: "Very Good Rock",
            2: "Good Rock",
            3: "Fair Rock",
            4: "Poor Rock",
            5: "Very Poor Rock"
        }
        rmr_class_name = rmr_classes.get(pred, "Unknown")

        # Grad-CAM ìƒì„±
        cam_image = generate_gradcam(model, input_tensor, original_np)

    # ì‹œê°í™”
    st.subheader(f"ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼: RMR Class {pred} ({rmr_class_name})")
    col1, col2 = st.columns(2)
    col1.image(image.resize((336, 336)), caption="ì›ë³¸ ì´ë¯¸ì§€", use_container_width=True)
    col2.image(cam_image, caption="Grad-CAM ì‹œê°í™”", use_container_width=True)

    # í´ë˜ìŠ¤ë³„ í™•ë¥  í‘œ ì¶œë ¥
    st.write("### í´ë˜ìŠ¤ë³„ í™•ë¥  (%)")
    prob_df = pd.DataFrame({
        "Class": [f"Class {i}" for i in range(1, 6)],
        "RMR ë“±ê¸‰": [rmr_classes[i] for i in range(1, 6)],
        "í™•ë¥  (%)": [round(p*100, 2) for p in probs]
    })
    st.table(prob_df)

    # ë°” ì°¨íŠ¸ ì‹œê°í™”
    st.bar_chart({
        "í™•ë¥  (%)": {f"Class {i} ({rmr_classes[i]})": p*100 for i, p in enumerate(probs, start=1)}
    })

    # GPT-5-mini ë¶„ì„ ì‹¤í–‰
    with st.spinner("GPT5-mini ë¶„ì„ ì¤‘..."):
        cam_pil = Image.fromarray(cam_image).resize((336, 336))
        original_resized = image.resize((336, 336))
        result = analyze_with_gpt(original_resized, cam_pil, pred, rmr_class_name)
        st.success("âœ… ë¶„ì„ ì™„ë£Œ")

    st.subheader("ğŸ§  GPT5-mini ê¸°ë°˜ ê¸°ìˆ  ë¶„ì„")
    st.markdown(result)

    # ë©”ëª¨ë¦¬ í•´ì œ
    gc.collect()
